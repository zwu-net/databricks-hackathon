{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Part 1 & 2: Data Ingestion\n",
    "# MAGIC \n",
    "# MAGIC This notebook ingests two data sources:\n",
    "# MAGIC 1.  **BLS Time Series Data:** Scrapes and syncs files from the BLS FTP-style website into the `bls_data` UC Volume.\n",
    "# MAGIC 2.  **US Population Data:** Fetches data from the DataUSA API and saves it to the `population_data` UC Volume.\n",
    "# MAGIC \n",
    "# MAGIC It's designed to be idempotentâ€”it won't re-download files that already exist and will remove files that are no longer in the source.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %pip install requests beautifulsoup4\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Setup Widgets and Paths\n",
    "# MAGIC \n",
    "# MAGIC Define the UC Volume paths.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# dbutils.widgets.text(\"catalog_name\", \"main\", \"Catalog Name\")\n",
    "# dbutils.widgets.text(\"schema_name\", \"hackathon\", \"Schema Name\")\n",
    "\n",
    "# catalog = dbutils.widgets.get(\"catalog_name\")\n",
    "# schema = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "# Using hardcoded values for simplicity in the bundle\n",
    "catalog = \"main\"\n",
    "schema = \"hackathon\"\n",
    "\n",
    "bls_volume_path = f\"/Volumes/{catalog}/{schema}/bls_data\"\n",
    "pop_volume_path = f\"/Volumes/{catalog}/{schema}/population_data\"\n",
    "\n",
    "# Create the directories if they don't exist (idempotent)\n",
    "os.makedirs(bls_volume_path, exist_ok=True)\n",
    "os.makedirs(pop_volume_path, exist_ok=True)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Part 1: BLS Time Series Data\n",
    "# MAGIC \n",
    "# MAGIC Scrape the BLS directory and sync files.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def sync_bls_data():\n",
    "    \"\"\"\n",
    "    Syncs files from the BLS data website to the UC Volume.\n",
    "    - Downloads new files.\n",
    "    - Deletes files from the volume that are no longer on the website.\n",
    "    \"\"\"\n",
    "    base_url = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "    \n",
    "    # Per the hackathon hint, a User-Agent is required to avoid 403 Forbidden errors.\n",
    "    # You should replace this with your own email or contact info.\n",
    "    headers = {\n",
    "        \"User-Agent\": \"DatabricksHackathonParticipant/1.0 (your-email@example.com)\"\n",
    "    }\n",
    "\n",
    "    print(f\"Syncing data from {base_url} to {bls_volume_path}...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Get list of files from BLS website\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all <a> tags, which link to files\n",
    "        source_files = set()\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            # Filter for actual data files (e.g., end in .txt, .data, .series)\n",
    "            if href and not href.startswith(('?', '/')):\n",
    "                source_files.add(href)\n",
    "        \n",
    "        print(f\"Found {len(source_files)} files at source.\")\n",
    "\n",
    "        # 2. Get list of files currently in the UC Volume\n",
    "        try:\n",
    "            volume_files = set(os.listdir(bls_volume_path))\n",
    "            print(f\"Found {len(volume_files)} files in volume.\")\n",
    "        except FileNotFoundError:\n",
    "            volume_files = set()\n",
    "            print(\"Volume directory not found or is empty.\")\n",
    "\n",
    "        # 3. Find files to add or update\n",
    "        files_to_download = source_files - volume_files\n",
    "        print(f\"Downloading {len(files_to_download)} new files...\")\n",
    "        for i, filename in enumerate(files_to_download):\n",
    "            file_url = f\"{base_url}{filename}\"\n",
    "            local_path = os.path.join(bls_volume_path, filename)\n",
    "            \n",
    "            try:\n",
    "                file_response = requests.get(file_url, headers=headers)\n",
    "                file_response.raise_for_status()\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(file_response.content)\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"  Downloaded {i + 1}/{len(files_to_download)}...\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"  Failed to download {filename}: {e}\")\n",
    "\n",
    "        # 4. Find files to delete\n",
    "        files_to_delete = volume_files - source_files\n",
    "        print(f\"Deleting {len(files_to_delete)} stale files...\")\n",
    "        for filename in files_to_delete:\n",
    "            local_path = os.path.join(bls_volume_path, filename)\n",
    "            os.remove(local_path)\n",
    "            print(f\"  Deleted {filename}\")\n",
    "\n",
    "        print(\"BLS data sync complete.\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to access BLS website: {e}\")\n",
    "        raise\n",
    "\n",
    "sync_bls_data()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Part 2: US Population API Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def fetch_population_data():\n",
    "    \"\"\"\n",
    "    Fetches US population data from the DataUSA API and saves it as a single JSON file.\n",
    "    \"\"\"\n",
    "    api_url = \"https://datausa.io/api/data?drilldowns=Nation&measures=Population\"\n",
    "    output_path = os.path.join(pop_volume_path, \"population_data.json\")\n",
    "\n",
    "    print(f\"Fetching population data from {api_url}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "            \n",
    "        print(f\"Successfully saved population data to {output_path}\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch population data: {e}\")\n",
    "        raise\n",
    "\n",
    "fetch_population_data()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Ingestion Complete\n",
    "# MAGIC \n",
    "# MAGIC Both datasets are now available in their respective UC Volumes.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbutils.notebook.exit(\"Data ingestion complete.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "666ba899894609f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
